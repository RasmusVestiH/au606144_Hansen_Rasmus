---
title: "Ancient cities and inscriptions"
author: "Rasmus Vesti Hansen"
date: "19/01/2021 updated `r format(Sys.time(), '%B %d, %Y')`" 
output:
  rmdformats::readthedown:
  highlight: kate
---

```{r setup, include=FALSE}
library(knitr)
library(rmdformats)

## Global options
options(max.print="75")
opts_chunk$set(echo=TRUE,
               cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```

In this exercise you will map the ancient equivalent of Twitter data: the ancient inscriptions. Ancient people of class, education, and means liked to advertise their achievements and life milestones as well as their sorrows via the means of texts inscribed in stone. These epigraphic monuments were often placed near inhabited areas, roads, and gathering places where they were likely to attract the largest audience. The location of these self-expressions in space and time is a reasonable indicator of changing economic prosperity of the commissioning communities. In this exercise, you will explore how these ancient inscriptions spatially correspond to the distribution of ancient cities and settlements.  

```{r libraries, include=FALSE}
library(sf)
library(raster)
library(tidyverse)
library(leaflet)
```

# Task 1: Load ancient cities and convert to sf object
John Hanson has created a dataset of all cities in the ancient Mediterranean and made it available online. You will download this dataset and convert it into an sf object in order to compare with the inscriptions on the basis of location.  

* Use `read_csv()` to load `Hanson2016_Cities_OxREP.csv` dataset from the provided URL and assign it to `cities` object

```{r load-cities, eval=TRUE}
cities <- as.data.frame(read_csv("http://oxrep.classics.ox.ac.uk/oxrep/docs/Hanson2016/Hanson2016_Cities_OxREP.csv"))
```


... then reproject this data to EPSG 3035
```{r prj-cities, eval=TRUE}
# Convert the table into an sf object on the basis of X and Y columns
cities_sf <- st_as_sf(cities, coords = c("Longitude (X)", "Latitude (Y)"))


# Define the projection of Lat/Long coordinates as EPSG 4326
cities_sf4326<- st_set_crs(cities_sf, 4326)
#By defining it as 4326 we create the Mercator map which is a projected map. Then we can transform it to the 3035 as the data we have is in Europe. This makes the projection more precise. 

# Transform the projection to a 2D projection using EPSG 3035
cities_sf3035<- st_transform(cities_sf4326, 3035)

# Verify the projection is 'projected' not 'geographic'
head(cities_sf3035)
#By looking at the head of the cities_sf3035 we see that it says projected CRS meaning we have a PCS (projected coordinate system)

```


### Question 1: 
*What are the measurement units of the `cities_sf3035` object?*
By running st_crs for the object we see the parameters of the length in easting and northing of respectively 4310000 and 3210000 in the unit of meters. This means that the object is 4321000 meters easting and 3210000 meters northing. 
```{r}
st_crs(cities_sf3035)

```


# Task 2: Create a buffer around each city and inspect the result

As each city and inscription corresponds to a dot on the map, the best way to grab and review the inscriptions will be by creating a buffer around each city point and then selecting inscriptions on the basis of that. 

* Create a buffer around the projected `cities` geometry with `st_buffer()` , setting the `dist` argument to the desired radius of 5000m.
* Plot the resulting buffer with city on top for quick review. 

```{r buff, eval=TRUE}
# Make buffer of 5 km. Check the units of your object to correctly assign value to dist
cities_5km<- st_buffer(cities_sf3035,dist = 5000)

cities_5km #Checking what the cities_5km consists of. 

# Plot the first 10 buffers and cities to check result 
plot(st_geometry(cities_5km)[1:10], col = "yellow")
plot(st_geometry(cities_sf3035)[1:10], pch=20, cex = 0.1, add = TRUE)

#We can plot the cities map to see how the overall structure is, which helps give an idea of what to expect.
plot(cities_sf3035)


#Now we have created the top 10 cities of the dataset and it looks ok. 
```


# Task 3: Verify the city buffers are indeed 5km in radius
Well, a quick review may look ok, but you cannot be sure your buffers work well until you add them to a map with a scale. Verify that your buffers are as big as should be by plotting a sample with tmap and adding a scale of good resolution.

* Grab the first 10 cities and buffers with slice() function
* Load tmap package and plot the 10 cities and buffers with a scale of 0,10,20km. Add names and background for clarity. Do your buffers span 10km across or do they span the universe? (If the latter, recheck your CRS, units, and dist argument)

```{r tmap, eval=TRUE}
# Grab the first 10 elements in the sf object and the buffer
ten_buffers <- cities_5km %>% slice(1:10)
ten_cities <- cities_sf3035 %>% slice(1:10)

# Create a quick tmap
library(tmap)
current.mode <- tmap_mode("plot")

tm_shape(ten_buffers)  +
  tm_polygons(col = "white") + #Use buffers for polygons in the color white
  tm_shape(ten_cities) +
  tm_text("Ancient Toponym", size = 0.7, auto.placement = 5) + #Make the text size of the text to 0.7
  tm_dots(col = "black", #cities as dots in black
             size = 0.1) +
  tm_scale_bar(breaks = c(0,10,20), #Setting the scale
               text.size = 10,
               position = c("RIGHT", "bottom")) +
  tm_compass(position = c("LEFT", "bottom"),
             type = "rose", 
             size = 2) +
  tm_credits(position = c("RIGHT", "top"),
             text = "R.Vesti, 2021") +
  tm_layout(main.title = "Map with a scale",
            bg.color = "beige",
            inner.margins = c(0, 0, 0, 0))

```


If all went well, you should see a map, where the diameter of each city buffer corresponds to the 10km notch on the scale
            
# Task 4: Download ancient inscriptions and wrangle coordinates into shape 
Let's now look at some data that spatially co-occurs with these ancient places. Below is a link to an online dataset from the Epigraphic Database of Heidelberg of ancient inscriptions from one part of the ancient world. These inscriptions combine private and official expressions dedicated for personal reasons (death of a dear person) or public (dedication of a major building, placement of milestone, etc.). 

The json dataset is hefty with some 12 thousand inscriptions and 74 variables. Coordinates are nested in a single column and may need wrangling. Do tasks deliberately in small steps after you test on subsets lest you overwhelm your R.

* Download the linked file with `download.file()` where you can find it. 
* The inscriptions dataset is in `.json` format, which is becoming the dominant format for sharing data online. Use the `jsonlite::fromJSON` function in the library to load it back into R
* Next, use `as_tibble()` to convert into rectangular format.  
* Check the column names looking for something that holds spatial data. There should be a `coordinates` column. Look at the column whether it holds meaningful coordinates.
* Separate the two values inside single coordinate column and create a separate longitude and a latitude column, which contain clean decimal numbers. You will need to clean up non-numeric characters en route. Make sure to keep the decimal point. Hint: there are lots of ways of getting clean decimal coordinates into two new columns, so feel free to diverge from the suggested course. Check out the `gsub()`, `grep()` and `str_extract()` functions to implement regular expressions in tidyverse pipeline. 

```{r inscriptions, eval=TRUE}
# Libraries
library(tidyverse)
library(jsonlite)
library(tidytext)

# Download the file and save as inscriptions.json (consider commenting out after you first run to avoid repeat downloading)
#We comment it out to prevent downloading it again: 
#download.file("https://sciencedata.dk/public/b6b6afdb969d378b70929e86e58ad975/EDH_subset_2021-02-15.json", "../HW03/inscriptions.json")

# Load it into R from wherever you put it, and convert into a tibble
list_json <- jsonlite::fromJSON("../HW03/inscriptions.json")
inscriptions = as_tibble(list_json)

# Check the first couple lines and column names
colnames(inscriptions) #First we look at what colnames we have
glimpse(inscriptions) #Glimpse let's us have a quick look on the data  
view(inscriptions) #While with the view function it opens a tab with the dataset to see all
head(inscriptions$coordinates) #At last we look at the coordinates column with head

# Wrangle the coordinates into a plottable  format
i_sm <- inscriptions %>% 
  slice(1:100) %>% 
  separate(col = coordinates, into = c("longitude","latitude"), sep = ",") %>%
  mutate(latitude = gsub(" ","", latitude), #Gsub extracts the space
         latitude = gsub(")","", latitude), #Gsub extracts the parentheses in the end
         longitude = gsub("c", "", longitude), #Gsub extracts the "c" in 
         longitude = gsub("\\(", "", longitude),#Gsub extracts the parentheses in the beginning
         latitude = as.numeric(latitude), #Changing the columns to numeric
         longitude = as.numeric(longitude))

#With the complete.cases we sort out the rows which have NA as coordinates values. So in case we need other information in the data set we would have to rerun the code above. But for a more clean result I prefer to exclude the NA data.
i_sm <- i_sm[complete.cases(i_sm[ , "latitude"]),]

#Here we check to see if the latitude and longitude data has been cleaned and can be used
i_sm$latitude
i_sm$longitude
#By looking at the coordinates we can also see that a lot of the coordinates replicates meaning we have many coords in the same position. Which is why the data set consisting of 97 coordinates only amounts to some 20 point in the leaflet below. 

# Check the result of the subset, does the location look reasonable?
leaflet() %>% addTiles() %>% addMarkers(lng=i_sm$longitude,lat=i_sm$latitude)
```

Oooof. That was some serious wrangling! 

### Question 2: 
*Which part of the world are the inscriptions from?*
By looking at the map we can see it is Italy.

# Task 5: Convert inscriptions into an sf object
Now that the hard work is done, let's apply the wrangling to the full dataset and clean up the missing coordinates and outlier values.

* Not all coordinates are complete. Remove the rows with missing latitude or longitude
* Some incorrect points have sneaked in! Eliminate data with longitude smaller than 5 and larger than 20 degrees.
* Make the resulting `inscriptions` tibble into an sf object using the newly created and cleaned longitude and latitude column in the `coords` argument. The CRS of the data is 4326.
* Plot your data using st_geometry()

```{r insc-sf, eval=TRUE}
i <- inscriptions %>% 
  separate(col = coordinates, into = c("longitude","latitude"), sep = ",") %>%
  mutate(latitude = gsub(" ","", latitude), #Gsub extracts the space
         latitude = gsub(")","", latitude), #Gsub extracts the parentheses in the end
         longitude = gsub("c", "", longitude), #Gsub extracts the "c" in 
         longitude = gsub("\\(", "", longitude),#Gsub extracts the parentheses in the beginning
         latitude = as.numeric(latitude), #Changing the columns to numeric
         longitude = as.numeric(longitude)) %>%
  filter(!is.na(longitude)) %>% #Filtering out the NA values in longitude and latitude
  filter(!is.na(latitude)) %>% 
  filter(longitude > 5 && longitude < 20) #Filtering the longitudes with less than 5 and more than 20 degrees

#I think this also could have done the trick instead of the filtering, but here we will do as suggested, just to be sure. So you don't need to run it, but if you do you will see that i doesn't change with the 12070 objects and 75 variables. 
i[complete.cases(i[ , "latitude"]),]


# Create a sf object
insc_sf4326 <- st_as_sf(i, coords = c("longitude", "latitude"), crs = 4326)

#We use the coords data we just created as longitude and latitude

# Plot
plot(st_geometry(insc_sf4326))

#Here we see two point that are further east and west than the cluster which is Italy
```

# Task 6: Select inscriptions that fall into the cities' buffer
Now that you have both the cities and inscriptions in the same CRS, you can pick the inscriptions which fall within 5km radius of the ancient places in order to locate "urban" inscriptions. Use the inverse st_difference to locate "rural" inscriptions.

To reduce the computational intensity of the final intersection, it is a good idea to limit the dissolved city buffer object only to the area within the convex hull of the inscriptions. For the convex hull, you will need to combine the inscriptions into a MULTIPOINT feature using `st_union()`. 

* Ensure that the spatial reference system in `cities_5km` buffer object and `inscriptions` is consistent.
* Create a convex hull for the inscriptions after combining them into a MULTIPOINT.
* Combine the city buffers into a single multipolygon
* Use `st_intersection()` to clip the inscriptions that fall within the buffer object and assign to `insc_urban` object
* Use `st_difference` flag to select inscriptions outside these buffers and create `insc_rural` object

```{r intersection, eval=TRUE}
# Project the sf object into EPSG3035 so it is consistent with cities and their buffers
crsdata <- st_crs(cities_sf3035) #we create the variable containing the crs for easier use in the script

insc_sf3035 <- st_transform(insc_sf4326, crs = crsdata)

crs(insc_sf3035)#By checking the crs we can see that they have the same crs
crs(cities_5km)


# Create a convex hull around the inscriptions's points dissolved into a MULTIPOINT
insc_ch <- st_convex_hull(st_union(insc_sf3035))

# Create a buffer from a cluster of cities that fall within the convex hull 
cities_it <- st_intersection(insc_ch, st_transform(cities_5km, crs = crsdata))


#Here we plot the objects to get an insight in to what is going on. 
plot(st_geometry(cities_it))
plot(st_geometry(insc_ch))
# I am not sure how we should get 399 buffers? But when running the above plots we see that we have created the convex hull surrounding our points as well as created buffers for the cities within. 
# Dissolve the 399 buffers into a single MULTIPOLYGON buffer feature
c_buff <- st_union(cities_it) #Here we unifies the plots to a multiplot


# Calculate the number of inscriptions in urban and rural areas. This may take a couple seconds
insc_urban <- st_intersection(insc_sf3035, c_buff)
insc_rural <- st_difference(insc_sf3035, c_buff)

plot(st_geometry(insc_urban))
plot(st_geometry(insc_rural))

#By dividing the id length of urban with rural we find the difference which is 6.5
length(insc_urban$id)/length(insc_rural$id)